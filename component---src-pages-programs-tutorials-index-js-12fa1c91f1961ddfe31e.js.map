{"version":3,"file":"component---src-pages-programs-tutorials-index-js-12fa1c91f1961ddfe31e.js","mappings":"kLA2mBA,EAzmBkB,WAChB,OACE,uBAAKA,UAAU,aACb,2BACE,sBAAIA,UAAU,sDAAd,cAIF,uBAAKA,UAAU,aACb,qBAAGA,UAAU,WAAb,kNAKA,qBAAGA,UAAU,sBAAb,mBACA,qBAAGA,UAAU,sBAAb,gFAIA,qBAAGA,UAAU,kBAAb,yFAIA,qBAAGA,UAAU,WAAb,mRAOA,qBAAGA,UAAU,WAAb,4gBAUA,qBAAGA,UAAU,WAAb,mbASA,qBAAGA,UAAU,WAAb,olBAWA,qBAAGA,UAAU,WAAb,iMAKA,qBAAGA,UAAU,qBAAb,6BACA,qBAAGA,UAAU,WACX,wBAAMA,UAAU,4BACd,qBAAGC,KAAK,sCAAsCC,OAAO,UAArD,0BAGM,IALV,iwBAmBA,qBAAGF,UAAU,WACX,wBAAMA,UAAU,4BACd,qBACEC,KAAK,iGACLC,OAAO,UAFT,uBAMM,IARV,+gBAkBA,qBAAGF,UAAU,WACX,wBAAMA,UAAU,4BACd,qBACEC,KAAK,sGACLC,OAAO,UAFT,2BAMM,IARV,0bAiBA,qBAAGF,UAAU,WACX,wBAAMA,UAAU,4BACd,qBACEC,KAAK,kGACLC,OAAO,UAFT,uBAMM,IARV,gTAeA,qBAAGF,UAAU,sBAAb,yFAIA,qBAAGA,UAAU,kBAAb,6EAIA,qBAAGA,UAAU,WAAb,+pBAaA,qBAAGA,UAAU,WAAb,0lBAWA,qBAAGA,UAAU,WAAb,mdASA,qBAAGA,UAAU,qBAAb,6BACA,qBAAGA,UAAU,WACX,wBAAMA,UAAU,aAAhB,mBADF,2aAUA,qBAAGA,UAAU,WACX,wBAAMA,UAAU,aAAhB,wBADF,+aAUA,qBAAGA,UAAU,WACX,wBAAMA,UAAU,aAAhB,iBADF,8bAUA,qBAAGA,UAAU,WACX,wBAAMA,UAAU,aAAhB,2BADF,umBAYA,qBAAGA,UAAU,sBAAb,sEAGA,qBAAGA,UAAU,kBAAb,+BACA,qBAAGA,UAAU,WAAb,iVAOA,qBAAGA,UAAU,WAAb,2zBAeA,qBAAGA,UAAU,WAAb,kSAOA,qBAAGA,UAAU,qBAAb,6BACA,qBAAGA,UAAU,WACX,wBAAMA,UAAU,aAAhB,qBADF,qmBAYA,qBAAGA,UAAU,WACX,wBAAMA,UAAU,aAAhB,cADF,6eAUA,qBAAGA,UAAU,sBAAb,qBACA,qBAAGA,UAAU,sBAAb,4DAGA,qBAAGA,UAAU,kBAAb,2CAGA,qBAAGA,UAAU,WAAb,yhCAkBA,qBAAGA,UAAU,WAAb,oCACoC,IAClC,sBAAIA,UAAU,4BACZ,4FAGA,sLAKA,8MAOJ,qBAAGA,UAAU,WAAb,2WAQA,qBAAGA,UAAU,qBAAb,6BACA,qBAAGA,UAAU,WACX,wBAAMA,UAAU,4BACd,qBAAGC,KAAK,6BAA6BC,OAAO,UAA5C,eAGM,IALV,+zBAoBA,qBAAGF,UAAU,WACX,wBAAMA,UAAU,aAAhB,sBADF,uwBAeA,qBAAGA,UAAU,WACX,wBAAMA,UAAU,aAAhB,cADF,o2BAgBA,qBAAGA,UAAU,sBAAb,6CAGA,qBAAGA,UAAU,kBAAb,qFAIA,qBAAGA,UAAU,WAAb,68CAwBA,qBAAGA,UAAU,qBAAb,6BACA,qBAAGA,UAAU,WACX,wBAAMA,UAAU,4BACd,qBAAGC,KAAK,sCAAsCC,OAAO,UAArD,2BAGM,IALV,2vBAmBA,qBAAGF,UAAU,WACX,wBAAMA,UAAU,4BACd,qBAAGC,KAAK,6BAA6BC,OAAO,UAA5C,qBAGM,IALV,ghBAeA,qBAAGF,UAAU,WACX,wBAAMA,UAAU,aAAhB,gBADF,g0CAuBA,qBAAGA,UAAU,WACX,wBAAMA,UAAU,4BACd,qBAAGC,KAAK,uBAAuBC,OAAO,UAAtC,8BAGM,IALV,srBAkBA,qBAAGF,UAAU,sBAAb,iEAGA,qBAAGA,UAAU,kBAAb,kEAGA,qBAAGA,UAAU,WAAb,iQAIkD,IAChD,wBAAMA,UAAU,kBACd,qBACEC,KAAK,8CACLC,OAAO,UAFT,qBANJ,2KAiBA,qBAAGF,UAAU,WAAb,ilBAWA,qBAAGA,UAAU,WAAb,uhBAUA,qBAAGA,UAAU,WACX,wBAAMA,UAAU,4BACd,qBAAGC,KAAK,6BAA6BC,OAAO,UAA5C,oBAGM,IALV,yzCA2BA,qBAAGF,UAAU,WACX,wBAAMA,UAAU,4BACd,qBAAGC,KAAK,qCAAqCC,OAAO,UAApD,iBACiB,MAEX,IALV,k1BAoBA,qBAAGF,UAAU,WACX,wBAAMA,UAAU,4BACd,qBAAGC,KAAK,2BAA2BC,OAAO,UAA1C,iBAGM,IALV,itBAkBA,qBAAGF,UAAU,WACX,wBAAMA,UAAU,4BACd,qBAAGC,KAAK,8BAA8BC,OAAO,UAA7C,oBAGM,IALV,4sBAqBP,EC3lBD,EAVc,WACZ,OACE,gBAAC,IAAD,KACE,uBAAKF,UAAU,6DACb,gBAAC,EAAD,OAIP,C","sources":["webpack://ismir-copy/./src/components/tutorials.js","webpack://ismir-copy/./src/pages/programs/tutorials/index.js"],"sourcesContent":["import React from \"react\";\r\n\r\nconst Tutorials = () => {\r\n  return (\r\n    <div className=\"space-y-5\">\r\n      <div>\r\n        <h1 className=\"text-[#d83616] font-bold md:text-4xl text-3xl mt-5\">\r\n          Tutorials\r\n        </h1>\r\n      </div>\r\n      <div className=\"space-y-3\">\r\n        <p className=\"text-xl\">\r\n          All the tutorials at ISMIR 2022 will take place on 04 December, 2022\r\n          in a hybrid format. There will be three parallel tutorials each in the\r\n          morning and the afternoon sessions, with a total of six tutorials.\r\n        </p>\r\n        <p className=\"text-3xl font-bold\">Morning Session</p>\r\n        <p className=\"text-2xl font-bold\">\r\n          T1(M): An Introduction to Symbolic Music Processing in Python with\r\n          Partitura\r\n        </p>\r\n        <p className=\"text-xl italic\">\r\n          Carlos Cancino-Chacón, Francesco Foscarin, Emmanouil Karystinaios,\r\n          Silvan David Peter\r\n        </p>\r\n        <p className=\"text-xl\">\r\n          Symbolic music formats (e.g., MIDI, MusicXML/MEI) can provide a\r\n          variety of high-level musical information like note pitch and\r\n          duration, key/time signature, beat/downbeat position, etc. Such data\r\n          can be used as both input/training data and as ground truth for MIR\r\n          systems.\r\n        </p>\r\n        <p className=\"text-xl\">\r\n          This tutorial aims to provide an introduction to symbolic music\r\n          processing for a broad MIR audience, with a particular focus on\r\n          showing how to extract relevant MIR features from symbolic musical\r\n          formats in a fast, intuitive, and scalable way. We do this with the\r\n          aid of the Python package Partitura. To target different kinds of\r\n          symbolic data, we use an extended version of the ASAP Dataset, a\r\n          multi-modal dataset that contains MusicXML scores, MIDI performances,\r\n          audio performances, and score-to-performance alignments.\r\n        </p>\r\n        <p className=\"text-xl\">\r\n          The tutorial will be structured in four parts: The first part provides\r\n          an introduction to the topic of symbolic music processing. The second,\r\n          third, and fourth parts are hands-on tutorials that showcase the\r\n          structure of the Partitura package (including its relation to other\r\n          popular Python packages for symbolic music processing), how to extract\r\n          common MIR features, and how to work with symbolic multimodal\r\n          datasets, respectively.\r\n        </p>\r\n        <p className=\"text-xl\">\r\n          The motivation behind this tutorial is to promote research on symbolic\r\n          music processing in the MIR community. Therefore, we target a broad\r\n          audience of researchers without requiring prior knowledge of this\r\n          particular area. For the hands-on parts of the tutorial, we presuppose\r\n          some practical experience with the Python language, but we will\r\n          provide well-documented step-by-step access to the code in the form of\r\n          Google Colab notebooks, which will be made publicly available after\r\n          the tutorial. Furthermore, some familiarity with the basic concepts of\r\n          statistics and machine learning is useful.\r\n        </p>\r\n        <p className=\"text-xl\">\r\n          This work receives funding from the European Research Council (ERC)\r\n          under the European Union’s Horizon 2020 research and innovation\r\n          programme, grant agreement No 101019375 (Whither Music?).\r\n        </p>\r\n        <p className=\"text-xl font-bold\">Biographies of Presenters</p>\r\n        <p className=\"text-xl\">\r\n          <span className=\"text-[#d83616] font-bold\">\r\n            <a href=\"http://www.carloscancinochacon.com/\" target=\"_blank\">\r\n              Carlos Cancino-Chacón\r\n            </a>\r\n          </span>{\" \"}\r\n          is an Assistant Professor at the Institute of Computational\r\n          Perception, Johannes Kepler University, Linz, Austria, and a Guest\r\n          Researcher at the RITMO Centre for Interdisciplinary Studies in\r\n          Rhythm, Time and Motion, University of Oslo, Norway. His research\r\n          focuses on studying expressive music performance, music cognition, and\r\n          music theory with machine learning methods. He received a doctoral\r\n          degree in Computer Science at the Institute of Computational\r\n          Perception of the Johannes Kepler University Linz, a M.Sc. degree in\r\n          Electrical Engineering and Audio Engineering from the Graz University\r\n          of Technology, a degree in Physics from the National Autonomous\r\n          University of Mexico, and a degree in Piano Performance from the\r\n          National Conservatory of Music of Mexico.\r\n        </p>\r\n        <p className=\"text-xl\">\r\n          <span className=\"text-[#d83616] font-bold\">\r\n            <a\r\n              href=\"https://www.jku.at/en/institute-of-computational-perception/about-us/people/franceso-foscarin/\"\r\n              target=\"_blank\"\r\n            >\r\n              Francesco Foscarin\r\n            </a>\r\n          </span>{\" \"}\r\n          is a postdoctoral researcher at the Institute of Computational\r\n          Perception, Johannes Kepler University, Linz, Austria. He completed\r\n          his Ph.D. at CNAM Paris on music transcription, with a focus on the\r\n          production of musical scores, and holds classical and jazz piano\r\n          degrees from the Conservatory of Vicenza. His research interests\r\n          include post-hoc explainability techniques for DL models,\r\n          grammar-based parsing of hierarchical chord structures, piano comping\r\n          generation for jazz music, and voice separation in symbolic music.\r\n        </p>\r\n        <p className=\"text-xl\">\r\n          <span className=\"text-[#d83616] font-bold\">\r\n            <a\r\n              href=\"https://www.jku.at/en/institute-of-computational-perception/about-us/people/emmanouil-karystinaios/\"\r\n              target=\"_blank\"\r\n            >\r\n              Emmanouil Karystinaios\r\n            </a>\r\n          </span>{\" \"}\r\n          is a Ph.D. student at the Institute of Computational Perception,\r\n          Johannes Kepler University, Linz, Austria. His research topics\r\n          encompass graph neural networks, music structure segmentation, and\r\n          automated music analysis. He holds an M.Sc. degree in Mathematical\r\n          Logic from Paris Diderot University, an M.A. in Composition from Paris\r\n          Vincennes University, and an integrated M.A. in Musicology from the\r\n          Aristotle University of Thessaloniki.\r\n        </p>\r\n        <p className=\"text-xl\">\r\n          <span className=\"text-[#d83616] font-bold\">\r\n            <a\r\n              href=\"https://www.jku.at/en/institute-of-computational-perception/about-us/people/silvan-david-peter/\"\r\n              target=\"_blank\"\r\n            >\r\n              Silvan David Peter\r\n            </a>\r\n          </span>{\" \"}\r\n          is a University Assistant at the Institute of Computational\r\n          Perception, Johannes Kepler University, Linz, Austria. His research\r\n          interests are the evaluation of and interaction with computational\r\n          models of musical skills. He holds an M.Sc. degree in Mathematics from\r\n          the Humboldt University of Berlin.\r\n        </p>\r\n        <p className=\"text-2xl font-bold\">\r\n          T2(M): Computational Methods For Supporting Corpus-Based Research On\r\n          Indian Art Music\r\n        </p>\r\n        <p className=\"text-xl italic\">\r\n          Thomas Nuttall, Genís Plaja-Roglans, Lara Pearson, Brindha\r\n          Manickavasakan\r\n        </p>\r\n        <p className=\"text-xl\">\r\n          Culture-aware approaches to computational musicology and music\r\n          information research (MIR) have been shown to be effective for a\r\n          musically relevant analysis of a music culture. Projects such as\r\n          CompMusic (2011-2017), MusicalBridges (2018-2022) or the initiatives\r\n          funded by SPARC (2019-2022) have demonstrated the importance of\r\n          considering sociocultural specifics of a music tradition to\r\n          effectively define research problems, collect data and propose methods\r\n          for analysis. These projects have made particularly notable\r\n          contributions to the analysis of Indian Art Music (IAM), leading to a\r\n          collective body of bespoke computational methods for analyzing these\r\n          traditions.\r\n        </p>\r\n        <p className=\"text-xl\">\r\n          Through this tutorial we aim to compile and present such works, making\r\n          openly available a number of software tools and materials developed by\r\n          MIR researchers working on the two main IAM traditions, Carnatic and\r\n          Hindustani. The content will be organized into five sections: (1)\r\n          datasets and corpora, (2) melodic analysis, (3) rhythmic analysis, (4)\r\n          timbral analysis and (5) structural analysis. Each topic will include\r\n          an introduction covering the basic musical concepts required to\r\n          understand its constituent tasks, followed by a practical presentation\r\n          of the materials and software tools compiled.\r\n        </p>\r\n        <p className=\"text-xl\">\r\n          This tutorial is the result of an ongoing collaborative effort\r\n          involving many contributors. The software will be available in Python\r\n          through a single Github repository, containing clear and reproducible\r\n          implementations of the presented methodologies. A Jupyter WebBook will\r\n          be the main tutorial reference, in which we will introduce all the\r\n          materials, contextualize the software tools, and include Jupyter\r\n          Notebook examples for most of the research tasks covered.\r\n        </p>\r\n        <p className=\"text-xl font-bold\">Biographies of Presenters</p>\r\n        <p className=\"text-xl\">\r\n          <span className=\"font-bold\">Thomas Nuttall </span>is a Research\r\n          Engineer in the Music Technology Group (MTG) of Universitat Pompeu\r\n          Fabra in Barcelona, Spain. His research focus is on melodic pattern\r\n          analysis in musical traditions under-represented in the music\r\n          computation and computational musicology fields, such as\r\n          Arab-Andalusian or Indian Art Music, and on building tools that bridge\r\n          the gap between the music information retrieval and musicology\r\n          research communities.\r\n        </p>\r\n        <p className=\"text-xl\">\r\n          <span className=\"font-bold\">Genís Plaja-Roglans </span>is a Ph.D\r\n          student in the Music Technology Group (MTG) of Universitat Pompeu\r\n          Fabra in Barcelona, Spain. His research focus is on creation of\r\n          bespoke machine learning models for the understanding of musical\r\n          traditions under-represented in Music Information Research, currently\r\n          focusing on Carnatic and Hindustani music. Recent work includes vocal\r\n          melody estimation, singing voice source separation and repeated\r\n          pattern discovery.\r\n        </p>\r\n        <p className=\"text-xl\">\r\n          <span className=\"font-bold\">Lara Pearson </span>is a musicologist at\r\n          the Max Planck Institute for Empirical Aesthetics (MPIEA), Frankfurt\r\n          am Main, Germany. Her work explores bodily and movement dimensions of\r\n          music experience and meaning, often combining sonic and kinetic\r\n          analyses. Her stylistic focus lies in South Indian music practices, in\r\n          particular Carnatic music. She has also published on cross-cultural\r\n          aesthetics, cultural heritage, music notation and the concept of\r\n          improvisation.\r\n        </p>\r\n        <p className=\"text-xl\">\r\n          <span className=\"font-bold\">Brindha Manickavasakan </span>is a\r\n          Carnatic Music vocalist, and is among the foremost, popular young\r\n          performing Carnatic musicians in India. She has been performing for\r\n          the past 21 years, and is currently learning from Vidushi Suguna\r\n          Varadachari. She is an ‘A’ graded vocal artist of All India Radio.\r\n          Brindha holds a Master’s degree in Biostatistics from Georgetown\r\n          University, USA, a Master’s degree in Music and is a PhD candidate in\r\n          Music from Madras University with a thesis on the contribution of\r\n          Tañjāvūr K Poṉṉayyā Piḷḷai. She is a constant feature in all the major\r\n          sabhas in Chennai, and performs regularly across India and abroad.\r\n        </p>\r\n        <p className=\"text-2xl font-bold\">\r\n          T3(M): Designing Controllable Synthesis System for Musical Signals\r\n        </p>\r\n        <p className=\"text-xl italic\">Hyeong-Seok Choi, Yusong Wu</p>\r\n        <p className=\"text-xl\">\r\n          Advances in deep learning and signal processing research have made it\r\n          possible to generate signals that at times can be difficult to\r\n          distinguish from real samples. Despite the realistic output the models\r\n          can produce, however, the controllability of the models is still\r\n          constrained because of the black-box-like nature of many models.\r\n        </p>\r\n        <p className=\"text-xl\">\r\n          In this tutorial, we aim to introduce considerations researchers can\r\n          take into account for a better end-user experience. We would like to\r\n          focus in particular on how to design deep generative models with\r\n          intuitive control of music audio signals, specifically vocal and\r\n          instrumental performance. To this end, we will first present a broad\r\n          review of up-to-date generative models for singing voices and musical\r\n          instrument performance. Then, we will share our own research results\r\n          and insights regarding both the implicit and explicit controllability\r\n          of the deep learning models. In the section on presenting controllable\r\n          models for instrumental performance synthesis, we will include a\r\n          walk-through of the building, training, and control of the DDSP and\r\n          MIDI-DDSP models via Jupyter (Colab) Notebook with Python and\r\n          Tensorflow.\r\n        </p>\r\n        <p className=\"text-xl\">\r\n          The target audience for this tutorial is researchers who are\r\n          interested in deep generative models for monophonic signals,\r\n          especially for singing voice and musical instruments. We expect the\r\n          audience to have a basic understanding of machine learning concepts\r\n          for audio signal processing.\r\n        </p>\r\n        <p className=\"text-xl font-bold\">Biographies of Presenters</p>\r\n        <p className=\"text-xl\">\r\n          <span className=\"font-bold\">Hyeong-Seok Choi </span> received his PhD\r\n          from Seoul National University, South Korea, in 2022, with a thesis\r\n          titled, “A Controllable Generation of Signals from Self-Supervised\r\n          Representations” under the supervision of Prof. Kyogu Lee. His recent\r\n          research interest is mainly in representation learning and\r\n          controllable synthesis of speech and singing voices. He co-founded the\r\n          audio technology startup company, Supertone, where he has been working\r\n          as the lead of their research team. He contributed to the winning of\r\n          the CES 2022 Innovation Awards Honoree: Software & Mobile Apps by\r\n          proposing a real-time voice conversion technology.\r\n        </p>\r\n        <p className=\"text-xl\">\r\n          <span className=\"font-bold\">Yusong Wu </span> is a final-year research\r\n          master at the University of Montreal and Mila in Montreal, Canada. He\r\n          is co-advised by Prof. Aaron Courville and Prof. Cheng-Zhi Anna Huang\r\n          and will become a Ph.D. student under the same advisors shortly. His\r\n          research focuses on making better generative models for music\r\n          creativity. His recent work ``MIDI-DDSP: Detailed Control of Musical\r\n          Performance via Hierarchical Modeling”, collaborating with Google\r\n          Magenta, was accepted by ICLR 2022 for oral presentation.\r\n        </p>\r\n        <p className=\"text-3xl font-bold\">Afternoon Session</p>\r\n        <p className=\"text-2xl font-bold\">\r\n          T4(A): Few-Shot and Zero-Shot Learning for Musical Audio\r\n        </p>\r\n        <p className=\"text-xl italic\">\r\n          Yu Wang, Hugo Flores García, Jeong Choi\r\n        </p>\r\n        <p className=\"text-xl\">\r\n          While deep neural networks achieved promising results in many MIR\r\n          tasks, they typically require a large amount of labeled data for\r\n          training. Rare, fine-grained, or newly emerged classes (e.g. a rare\r\n          musical instrument, a new music genre) where large-scale data\r\n          collection is hard or simply impossible are often considered\r\n          out-of-vocabulary and unsupported by MIR systems. To address this,\r\n          few-shot learning (FSL) and zero-shot learning (ZSL) are learning\r\n          paradigms that aim to train a model that can learn a new concept based\r\n          on just a handful of labeled examples (few-shot) or some auxiliary\r\n          information (zero-shot), mimicking human ability. By doing so, the\r\n          trained model is no longer limited to a pre-defined and fixed set of\r\n          classes but ideally can generalize to any class of interest with the\r\n          cost of little human intervention. In addition, few-shot and zero-shot\r\n          models naturally incorporate human input without asking for\r\n          significant effort, making them useful tools when developing MIR\r\n          systems that can be customized by individual users.\r\n        </p>\r\n        <p className=\"text-xl\">\r\n          In this tutorial, we will go over{\" \"}\r\n          <ul className=\"list-decimal list-inside\">\r\n            <li>\r\n              FSL/ZSL foundations - Task definition and existing approaches.\r\n            </li>\r\n            <li>\r\n              Recent advances of FSL/ZSL in MIR - Techniques and contributions\r\n              in recent studies. We will also discuss the remaining challenges\r\n              and future directions.\r\n            </li>\r\n            <li>\r\n              Coding examples - Showcasing the training and evaluation pipeline\r\n              of FSL and ZSL models on specific MIR tasks. Code and references\r\n              to the tools and datasets will be provided.\r\n            </li>\r\n          </ul>\r\n        </p>\r\n        <p className=\"text-xl\">\r\n          We aim for this tutorial to be useful to researchers and practitioners\r\n          in the ISMIR community who are facing labeled data scarcity issues,\r\n          looking for new interaction paradigms between users and MIR systems,\r\n          or generally interested in the techniques and applications of FSL and\r\n          ZSL. We assume the audience is familiar with the basic machine\r\n          learning concepts.\r\n        </p>\r\n        <p className=\"text-xl font-bold\">Biographies of Presenters</p>\r\n        <p className=\"text-xl\">\r\n          <span className=\"text-[#d83616] font-bold\">\r\n            <a href=\"https://y-wang.weebly.com/\" target=\"_blank\">\r\n              Yu Wang Yu\r\n            </a>\r\n          </span>{\" \"}\r\n          is a Ph.D. candidate in Music Technology at the Music and Audio\r\n          Research Laboratory at New York University, working under Prof. Juan\r\n          Pablo Bello. Her research interests focus on machine learning and\r\n          signal processing for music and general audio. Specifically, she is\r\n          interested in adaptive and interactive machine listening with minimal\r\n          supervision. She has interned with Adobe Research and Spotify. Before\r\n          joining MARL in 2017, she was in the Music Recording and Production\r\n          program at the Institute of Audio Research. She holds two M.S. degrees\r\n          in Materials Science & Engineering from Massachusetts Institute of\r\n          Technology (2015) and National Taiwan University (NTU) (2012), and a\r\n          B.S. in Physics from NTU (2010). Yu is a guitar player and also enjoys\r\n          sound engineering. Japanese math rock is her current favorite music\r\n          genre.\r\n        </p>\r\n        <p className=\"text-xl\">\r\n          <span className=\"font-bold\">Hugo Flores García</span> is a Ph.D.\r\n          student in Computer Science at Northwestern University, working under\r\n          Prof. Bryan Pardo in the Interactive Audio Lab. Hugo’s research\r\n          interests lie at the intersection of machine learning, signal\r\n          processing, and human computer interaction for music and audio. Hugo\r\n          has previously worked on a deep learning framework for Audacity, an\r\n          open source audio editor, and will be a research intern at Spotify and\r\n          Descript during the latter half of 2022. Hugo holds an B.S. in\r\n          Electrical Engineering from Georgia Southern University (2020). He is\r\n          a jazz guitarist, and can be seen playing with various groups local to\r\n          the Chicago area. Hugo enjoys augmenting musical instruments with\r\n          technology, as well as making interactive music and art in\r\n          SuperCollider and Max/MSP.\r\n        </p>\r\n        <p className=\"text-xl\">\r\n          <span className=\"font-bold\">Jeong Choi</span> is a machine learning\r\n          researcher at Naver, where he leads NOW AI team that’s working on a\r\n          multi-modal recommendation system for a video streaming service, Naver\r\n          NOW. Before joining Naver, he was a researcher at NCSOFT, working on a\r\n          recommedation system in a music game FUSER. He also interned at Deezer\r\n          Research. He received a M.S. in Culture Technology at Korea Advance\r\n          Institute of Science and Technology, under the supervision of Prof.\r\n          Juhan Nam. His research interest is on representational learning of\r\n          various signals that can further contribute to diverse music\r\n          recommendation strategies. Previously, he pursued a long music career\r\n          as a composer and a bassist. His passion for music research originates\r\n          from the experience. He also received a M.S. and a B.E. in Digital\r\n          Media at Ajou University, and majored in French at Daewon Foreign\r\n          Language High School.\r\n        </p>\r\n        <p className=\"text-2xl font-bold\">\r\n          T5(A): Deep learning for automatic mixing\r\n        </p>\r\n        <p className=\"text-xl italic\">\r\n          Christian J. Steinmetz, Soumya Sai Vanka, Gary Bromham, Marco A.\r\n          Martínez Ramírez\r\n        </p>\r\n        <p className=\"text-xl\">\r\n          Mixing is a central task within audio post-production where expert\r\n          knowledge is required to deliver professional quality content,\r\n          encompassing both technical and creative considerations. Recently,\r\n          deep learning approaches have been introduced that aim to address this\r\n          challenge by generating a cohesive mixture of a set of recordings as\r\n          would an audio engineer. These approaches leverage large-scale\r\n          datasets and therefore have the potential to outperform traditional\r\n          approaches based on expert systems, but bring their own unique set of\r\n          challenges. In this tutorial, we will begin by providing an\r\n          introduction to the mixing process from the perspective of an audio\r\n          engineer, along with a discussion of the tools used in the process\r\n          from a signal processing perspective. We will then discuss a series of\r\n          recent deep learning approaches and relevant datasets, providing code\r\n          to build, train, and evaluate these systems. Future directions and\r\n          challenges will be discussed, including new deep learning systems,\r\n          evaluation methods, and approaches to address dataset availability.\r\n          Our goal is to provide a starting point for researchers working in MIR\r\n          who have little to no experience in audio engineering so they can\r\n          easily begin addressing problems in this domain. In addition, our\r\n          tutorial may be of interest to researchers outside of MIR, but with a\r\n          background in audio engineering or signal processing, who are\r\n          interested in gaining exposure to current approaches in deep learning.\r\n        </p>\r\n        <p className=\"text-xl font-bold\">Biographies of Presenters</p>\r\n        <p className=\"text-xl\">\r\n          <span className=\"text-[#d83616] font-bold\">\r\n            <a href=\"https://www.christiansteinmetz.com/\" target=\"_blank\">\r\n              Christian J. Steinmetz\r\n            </a>\r\n          </span>{\" \"}\r\n          is PhD researcher working with Prof. Joshua D. Reiss within the Centre\r\n          for Digital Music at Queen Mary University of London. He researches\r\n          applications of machine learning in audio with a focus on\r\n          differentiable signal processing. Currently, his research revolves\r\n          around high fidelity audio and music production, which involves\r\n          enhancing audio recordings, intelligent systems for audio engineering,\r\n          as well as applications that augment and extend creativity. He has\r\n          worked as a Research Scientist Intern at Adobe, Facebook Reality Labs,\r\n          and Dolby Labs. Christian holds a BS in Electrical Engineering and BA\r\n          in Audio Technology from Clemson University, as well as an MSc in\r\n          Sound and Music Computing from the Music Technology Group at\r\n          Universitat Pompeu Fabra.\r\n        </p>\r\n        <p className=\"text-xl\">\r\n          <span className=\"text-[#d83616] font-bold\">\r\n            <a href=\"https://www.saisoumya.com/\" target=\"_blank\">\r\n              Soumya Sai Vanka\r\n            </a>\r\n          </span>{\" \"}\r\n          is a first year PhD researcher at the Centre for Digital Music, Queen\r\n          Mary University of London. She is part of the AI and Music, Centre for\r\n          Doctoral Training. Her research focus is mainly on exploring the idea\r\n          of Music Mix similarity, Music Mix Style transfer, and Intelligent\r\n          Multitrack Mixing using Self-Supervised, Semi-Supervised, and\r\n          Unsupervised Learning architectures. She also writes music, produces\r\n          and plays saxophone. Her educational background is a mixture of\r\n          Masters in Physics and Courses in Music Production.\r\n        </p>\r\n        <p className=\"text-xl\">\r\n          <span className=\"font-bold\">Gary Bromham</span> is a part-time PhD\r\n          researcher at Queen Mary University of London, researching the role\r\n          that traditional studio paradigms and retro aesthetics play in\r\n          intelligent music production systems (2016 -). He has several\r\n          publications in this field and has contributed a chapter to the recent\r\n          Routledge publication, ‘Perspectives on Music Production: Mixing\r\n          Music’ (2017). He was also a research assistant on the EPSRC funded\r\n          project called FAST (Fusing Audio and Semantic Technologies) where he\r\n          is employed as an industry advisor (2017 - 2020). In addition to his\r\n          research interests, Gary is a practising music producer, songwriter\r\n          and audio engineer, with over 30 years’ experience (1989 - 2020). He\r\n          has worked with artists as diverse as Bjork, Wham, Blur and U2, during\r\n          a period that has witnessed several technological changes. Gary is\r\n          well versed in most popular music making software and has extensive\r\n          knowledge of using analog hardware, acting as a product designer and\r\n          specialist for the renowned mixing desk company, Solid State Logic. He\r\n          is also a frequent guest lecturer and external advisor at several\r\n          universities in the UK, Norway and Sweden; speaking on songwriting,\r\n          music production aesthetics and audio engineering and bringing some of\r\n          his extensive knowledge and experience to both Undergraduate and\r\n          Master’s degree level programs.\r\n        </p>\r\n        <p className=\"text-xl\">\r\n          <span className=\"text-[#d83616] font-bold\">\r\n            <a href=\"https://m-marco.com/\" target=\"_blank\">\r\n              Marco A. Martínez Ramírez\r\n            </a>\r\n          </span>{\" \"}\r\n          is music technology researcher at Sony in the Tokyo R&D center, where\r\n          he is part of the Creative AI Lab. His research interests lie at the\r\n          intersection of machine learning, digital signal processing, and\r\n          intelligent music production, with a primary focus on deep learning\r\n          architectures for music processing tasks. Previously, he was an audio\r\n          research intern at Adobe and received his PhD from the Centre for\r\n          Digital Music at Queen Mary University of London. He has a MSc in\r\n          digital signal processing from the University of Manchester, UK, and a\r\n          BSc in electronic engineering from La Universidad de Los Andes,\r\n          Colombia. Marco also has a background in music production and mixing\r\n          engineering.\r\n        </p>\r\n        <p className=\"text-2xl font-bold\">\r\n          T6(A): Trustworthy MIR: Creating MIR applications with values\r\n        </p>\r\n        <p className=\"text-xl italic\">\r\n          Christine Bauer, Andrés Ferraro, Emilia Gómez, Lorenzo Porcaro\r\n        </p>\r\n        <p className=\"text-xl\">\r\n          The MIR community shows an increasing interest in understanding how\r\n          current technologies affect the everyday experience of people all over\r\n          the world, e.g., how we listen to music, compose songs, or learn to\r\n          play an instrument. As it was introduced in the{\" \"}\r\n          <span className=\"text-[#d83616]\">\r\n            <a\r\n              href=\"https://ismir2019.ewi.tudelft.nl/?q=node/41\"\r\n              target=\"_blank\"\r\n            >\r\n              FAT-MIR tutorial\r\n            </a>\r\n          </span>\r\n          held at ISMIR 2019, a great discussion has aroused around the ethical,\r\n          social, economic, legal, and cultural implications that the use of MIR\r\n          systems have in our life.\r\n        </p>\r\n        <p className=\"text-xl\">\r\n          In this tutorial, we aim at building upon and expanding the\r\n          aforementioned debate, discussing the more recent results obtained by\r\n          the MIR community and beyond. The goal of the tutorial is to show how\r\n          values, such as fairness and diversity, can be embedded in the life\r\n          cycle of MIR systems to make them trustworthy: from algorithmic design\r\n          to evaluation practices and regulatory proposals. To achieve that, we\r\n          will discuss examples of, among the others, popularity bias, gender\r\n          bias, algorithmic bias, music styles underrepresentation, and\r\n          diversity-related phenomena (e.g. filter bubbles).\r\n        </p>\r\n        <p className=\"text-xl\">\r\n          This tutorial is suitable for researchers and students in MIR working\r\n          in any domain, as these issues are relevant for all MIR tasks. The\r\n          examples will mostly focus on music information retrieval and\r\n          recommendation, but there are no prerequisites for taking this\r\n          tutorial. Besides presenting recent research insights, the tutorial\r\n          will integrate two hands-on sessions, where we will involve the\r\n          participants in reflecting on the design of evaluation methods that\r\n          take into account values for which MIR systems should be accountable.\r\n        </p>\r\n        <p className=\"text-xl\">\r\n          <span className=\"text-[#d83616] font-bold\">\r\n            <a href=\"https://christinebauer.eu/\" target=\"_blank\">\r\n              Christine Bauer\r\n            </a>\r\n          </span>{\" \"}\r\n          is an assistant professor at the Department of Information and\r\n          Computing Sciences at Utrecht University, The Netherlands. Her\r\n          research activities center on interactive intelligent systems.\r\n          Recently, she focuses on context-aware (music) recommender systems. A\r\n          core interest in her research activities are fairness in algorithmic\r\n          decision-making and multi-method evaluations. Her research and\r\n          teaching activities are driven by her interdisciplinary background.\r\n          She holds a Doctoral degree in Social and Economic Sciences, a MSc in\r\n          Business Informatics, and a Diploma degree in International Business\r\n          Administration. In addition, she pursued studies in jazz saxophone.\r\n          Christine holds several best paper awards and awards for her reviewing\r\n          activities. Furthermore, she received the Elise Richter grant by the\r\n          Austrian Science Fund. Before joining Utrecht University, she was a\r\n          researcher at Johannes Kepler University Linz, WU Wien, and EC3\r\n          (Austria), and University of Cologne (Germany). In 2013 and 2015, she\r\n          was a Visiting Fellow at Carnegie Mellon University (PA, USA).\r\n          Christine has co-organized the workshop PERSPECTIVES 2021 at RecSys\r\n          2021 and IUadaptMe 2019 at UMAP 2019. At UMAP 2021, she gave a\r\n          tutorial on Multi-Method Evaluation of Adaptive Systems. Furthermore,\r\n          she was a co-chair for the Doctoral Symposium at RecSys 2021.\r\n        </p>\r\n        <p className=\"text-xl\">\r\n          <span className=\"text-[#d83616] font-bold\">\r\n            <a href=\"https://andrebola.github.io/about/\" target=\"_blank\">\r\n              Andrés Ferraro{\" \"}\r\n            </a>\r\n          </span>{\" \"}\r\n          (BSc/MSc in Software Engineering) is a Postdoctoral Fellow at McGill\r\n          University and Mila (Quebec AI Institute), Canada. He completed his\r\n          PhD at the Department of Information and Communication Technologies\r\n          and Engineering of the Universitat Pompeu Fabra, Spain. His thesis\r\n          uncovers multiple dimensions in which music recommender systems affect\r\n          the artists and proposes alternatives to mitigate such problems. He is\r\n          currently part of an interdisciplinary project, rethinking music\r\n          recommender systems by considering new and alternative conceptions\r\n          from the social sciences and humanities, informed by non-profit\r\n          systems and critical debates over bias and discrimination. He is\r\n          co-organizer of LatAm Bish Bash, a series of meetings and networking\r\n          events that connect engineers, researchers, and students working on\r\n          music and audio signal processing.\r\n        </p>\r\n        <p className=\"text-xl\">\r\n          <span className=\"text-[#d83616] font-bold\">\r\n            <a href=\"https://emiliagomez.com/\" target=\"_blank\">\r\n              Emilia Gómez\r\n            </a>\r\n          </span>{\" \"}\r\n          (BSc/MSc in Electrical Engineering, PhD in Computer Science) is\r\n          Principal Investigator on Human and Machine Intelligence (HUMAINT)\r\n          team at the Joint Research Center (European Commission). She is also a\r\n          Guest Professor at the Music Technology Group, Universitat Pompeu\r\n          Fabra, Barcelona. Her research is grounded on the Music Information\r\n          Retrieval field, where she has developed data-driven technologies to\r\n          support music listening experiences. Starting from music, she studies\r\n          the impact of artificial intelligence (AI) on human decision making,\r\n          cognitive and socio-emotional development. Her research interests\r\n          include fairness and transparency in AI, the impact of AI on jobs, and\r\n          how it affects children development.\r\n        </p>\r\n        <p className=\"text-xl\">\r\n          <span className=\"text-[#d83616] font-bold\">\r\n            <a href=\"https://lorenzoporcaro.com/\" target=\"_blank\">\r\n              Lorenzo Porcaro\r\n            </a>\r\n          </span>{\" \"}\r\n          (MSc Sound and Music Computing and Intelligent Interactive Systems) is\r\n          a PhD candidate at the Music Technology Group, Universitat Pompeu\r\n          Fabra (UPF), Spain. His research is at the intersection between Music\r\n          Information Retrieval and Social Computing, and he is currently\r\n          working on the assessment of the impact of music recommender systems\r\n          on cultural diversity. He has collaborated in several initiatives\r\n          focused on the analysis of ethical dimensions of algorithmic systems\r\n          (Mechanism Design for Social Good (MD4SG); divinAI project, HUMAINT /\r\n          UPF). He has also been part of national and international research\r\n          projects aiming at making music more accessible through the use of\r\n          technology (Musical AI, TROMPA).\r\n        </p>\r\n      </div>\r\n    </div>\r\n  );\r\n};\r\n\r\nexport default Tutorials;\r\n","import React from 'react'\r\nimport Layout from '../../../components/layout'\r\nimport Tutorial from '../../../components/tutorials'\r\n\r\nconst Index = () => {\r\n  return (\r\n    <Layout>\r\n      <div className=\"flex flex-col space-y-5 md:mx-30 lg:mx-52 mx-5 text-left \">\r\n        <Tutorial/>\r\n      </div>\r\n    </Layout>\r\n  )\r\n}\r\n\r\nexport default Index\r\n"],"names":["className","href","target"],"sourceRoot":""}